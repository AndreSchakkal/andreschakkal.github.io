<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>André Schakkal</title>

    <meta name="author" content="André Schakkal">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
 	<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  André Schakkal
                </p>
                <p>
		I’m a robotics engineer specializing in robot manipulation, reinforcement learning, and foundation models. I recently completed my Master’s degree in Robotics,
		with a minor in Data Science, at <a href="https://www.epfl.ch/">EPFL</a> (2025), conducting my thesis at <a href="https://www.mit.edu/">MIT</a> under the supervision of
		<a href="https://azizan.mit.edu/">Navid Azizan</a> and <a href="https://zt-yang.com/">Zhutian Yang</a>. 
		My work at <a href="https://www.mit.edu/">MIT</a> focused on long-horizon humanoid manipulation using vision-language models and imitation learning.</p>
		<p>
		Throughout my Master’s, I worked on deploying
		learning-based control systems in both simulation and real-world robots, including quadrupeds and humanoids, collaborating with 
		<a href="https://www.epfl.ch/labs/biorob/people/ijspeert/">Auke Ijspeert</a> as well. Prior to that, I earned my BSc in Mechanical Engineering 
		from <a href="https://www.epfl.ch/">EPFL</a> (2022). I’m particularly interested in building intelligent systems that can reason, plan, and act over extended tasks, 
		and I’m open to opportunities in robotics research and applied AI.
                </p>
                <p style="text-align:center">
                  <a href="mailto:andre.schakkal@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/CVAndreSchakkal.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=L1sCqAkAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/AndreSchakkal/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/AndreSchakkal.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/AndreSchakkal.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			  
<tr onmouseout="longhorizon_stop()" onmouseover="longhorizon_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='longhorizon_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/longhorizon.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/longhorizon.jpg' width="160">
        </div>
        <script type="text/javascript">
          function longhorizon_start() {
            document.getElementById('longhorizon_image').style.opacity = "1";
          }

          function longhorizon_stop() {
            document.getElementById('longhorizon_image').style.opacity = "0";
          }
          longhorizon_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
			Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation
          <!-- <span class="strong">Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation</span> -->
        </strong>
        <br>
		<strong>André Schakkal</strong>,
        <a href="https://benzandonati.co.uk/">Ben Zandonati</a>,
        <a href="https://zt-yang.com/">Zhutian Yang</a>,
        <a href="https://azizan.mit.edu/">Navid Azizan</a>
        <br>
        <em>2025 Robotics: Science and Systems Conference (RSS) Workshop on Robot Planning in the Era of Foundation Models</em>, 2025
        <br>
        <a href="https://vlp-humanoid.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2506.22827">paper</a>
        <p></p>
        <p>
		This work presents a hierarchical framework that enables humanoid robots to autonomously execute multi-step manipulation tasks. The system combines a low-level RL controller
		for whole-body motion tracking, mid-level imitation-learned skills for generating action-specific motion targets, and a high-level vision-language module that sequences skills 
		and monitors execution in real time. Tested on a Unitree G1 humanoid performing a pick-and-place task, the system achieves a 73% success rate.
		<!-- This work introduces a hierarchical framework for multi-step humanoid manipulation. The system integrates three layers: (1) a low-level RL controller that 
		tracks whole-body motion targets; (2) a mid-level set of skill policies trained via imitation learning that produce motion targets for different steps of a task; and 
		(3) a high-level vision-language module that selects and monitors skills using pretrained VLMs.         -->
		</p>
      </td>
    </tr>
	
    <tr onmouseout="inverse_stop()" onmouseover="inverse_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='inverse_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/inverse.mp4" type="video/mp4">
          Your browser does not support the video tag.

          </video></div>
          <img src='images/inverse.jpg' width="160">
        </div>
        <script type="text/javascript">
          function bolt3d_start() {
            document.getElementById('inverse_image').style.opacity = "1";
          }

          function bolt3d_stop() {
            document.getElementById('inverse_image').style.opacity = "0";
          }
          inverse_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
		Learning the Inverse Hitting Problem
        </strong>
        <br>
        <a href="https://harshitk95.github.io/">Harshit Khurana</a>,
        <a href="https://jameshermus.github.io/">James Hermus</a>,
        <a href="https://www.linkedin.com/in/maxime-gautier-00/">Maxime Gautier</a>,
		<strong>André Schakkal</strong>,
        <a href="https://people.epfl.ch/aude.billard">Aude Billard</a>
        <br>
        <em>2025 IEEE Robotics and Automation Letters (RA-L)</em>, 2025
        <br>
        <a href="https://ieeexplore.ieee.org/document/11071941">paper</a>
        <p></p>
        <p>
			This work presents a data-driven framework for impulsive object manipulation, where robots move objects by hitting them. Using a dual-arm air-hockey setup, the system learns the relationship between hitting force
			and object motion via a Gaussian Mixture Model and predicts impacts with an Impact-Aware Extended Kalman Filter. The approach enables collaborative, long-range object placement through sequential, golf-like hits.
			
			<!-- This work introduces a data-driven framework for modeling and planning impulsive object manipulation—where robots move objects by hitting rather than pushing or grasping. Using a dual-arm “air-hockey” setup,
			the system autonomously collects data as two robot arms exchange impacts on an object. An Impact-Aware Extended Kalman Filter (IA-EKF) predicts post-impact motion online, while a Gaussian Mixture Model (GMM)
			learns the stochastic relationship between hitting force and object displacement. The learned model enables robots to plan sequential, golf-like hits to move objects beyond their individual reach, demonstrating
			collaborative long-range object placement. -->
        </p>
      </td>
    </tr>


  <tr onmouseout="quad_stop()" onmouseover="quad_start()">
      <!-- <td style="padding:16px;width:20%;vertical-align:middle"> -->
	  <td style="padding:16px;width:20%;vertical-align:middle; text-align:center;">
        <div class="one">
          <div class="two" id='quad_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/quad.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/quad.jpg' width="160">
        </div>
        <script type="text/javascript">
          function quad_start() {
            document.getElementById('quad_image').style.opacity = "1";
          }

          function quad_stop() {
            document.getElementById('quad_image').style.opacity = "0";
          }
          quad_stop()
        </script>
	  </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
		Dynamic Object Catching with Quadruped Robot Front Legs
        </strong>
        <br>
				<strong>André Schakkal</strong>, 
				<a href="https://www.linkedin.com/in/guillaume-bellegarda/">Guillaume Bellegarda</a>,
				<a href="https://www.epfl.ch/labs/biorob/people/ijspeert/">Auke Ijspeert</a>
				<br>
        <em>2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 
        <br>
        <a href="https://arxiv.org/abs/2410.08065v1">paper</a>
		/
		<a href="https://www.youtube.com/watch?v=sm7RdxRfIYg/">video</a>
        <p></p>
        <p>
			This work enables a quadruped robot to catch thrown objects using its front legs while standing on its rear legs. Using an onboard camera 
			and a fine-tuned YOLOv8 model, the robot detects objects, predicts their trajectories, and executes a precise catching maneuver. A Gaussian
			Mixture Model (GMM) identifies optimal catching positions, enabling an 80% success rate across diverse throws.
			<!-- This paper presents a framework for dynamic object catching using a quadruped robot's front legs while it stands on its rear legs. 
			The system integrates computer vision, trajectory prediction, and leg control to enable the quadruped to visually detect, track, and 
			successfully catch a thrown object using an onboard camera. -->
        </p>
      </td>
    </tr>


    <!-- <tr onmouseout="synergizing_stop()" onmouseover="synergizing_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="synergizing_stop()" onmouseover="synergizing_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='synergizing_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/synergizing.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/synergizing.jpg' width="160">
        </div>
        <script type="text/javascript">
          function synergizing_start() {
            document.getElementById('synergizing_image').style.opacity = "1";
          }

          function synergizing_stop() {
            document.getElementById('synergizing_image').style.opacity = "0";
          }
          synergizing_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
			Synergizing Natural Language Towards Enhanced Shared Autonomy
        </strong>
        <br>
				<a href="https://shalutharajapakshe.com/">Shalutha Rajapakshe</a>,
				<a href="https://www.linkedin.com/in/atharva-dastenavar/">Atharva Dastenavar</a>,
				<strong>André Schakkal</strong>,
				<a href="https://emmanuel-senft.github.io/">Emmanuel Senft</a>
        <br>
        <em>Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction (HRI)</em>, 2024
        <br>
        <a href="https://dl.acm.org/doi/10.1145/3610978.3640720">paper</a>
        <p></p>
        <p>
				 This work introduces a voice-based shared autonomy framework for assistive robots, enabling users to refine robot behavior through natural language instead of physical inputs like joysticks
			. A fine-tuned DistilBERT model interprets sequences of verbal commands to infer correction directions (e.g., left, right, up) and distinguish between environment-dependent and independent instructions.
			The lightweight model runs on a CPU, outperforming larger transformers on complex sequential commands, and lays the groundwork for voice-based corrections in shared autonomy systems.
        </p>
      </td>
    </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			  
<tr onmouseout="longhorizon_stop()" onmouseover="longhorizon_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='longhorizon_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/longhorizon.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/longhorizon.jpg' width="160">
        </div>
        <script type="text/javascript">
          function longhorizon_start() {
            document.getElementById('longhorizon_image').style.opacity = "1";
          }

          function longhorizon_stop() {
            document.getElementById('longhorizon_image').style.opacity = "0";
          }
          longhorizon_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
			Camera-Based Humanoid Teleoperation
        </strong>
        <br>
		<strong>André Schakkal</strong>,
        <a href="https://benzandonati.co.uk/">Ben Zandonati</a>,
        <a href="https://zt-yang.com/">Zhutian Yang</a>,
        <a href="https://azizan.mit.edu/">Navid Azizan</a>
        <br>
        <em>2025 Robotics: Science and Systems Conference (RSS) Workshop on Robot Planning in the Era of Foundation Models</em>, 2025
        <br>
        <a href="https://vlp-humanoid.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2506.22827">paper</a>
        <p></p>
        <p>
		Summary     
		</p>
      </td>
    </tr>			

			
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Honors and Awards</h2>
			    <ul>
			    <li> Faulhaber Best Master Thesis Award (2025)</li>
			    <li> EPFL Master Excellence Fellowship (2022-2025)</li>
			    </ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



			
          
					<!-- <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=hFlF33JZbA0">Radiance Fields and the Future of Generative Media, 2025</a><br>				  
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a><br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
</a><br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a><br>
				<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a><br>
				<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html> -->
