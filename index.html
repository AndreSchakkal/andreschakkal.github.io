<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>André Schakkal</title>

    <meta name="author" content="André Schakkal">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon_robot/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
 	<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  André Schakkal
                </p>
                <p>
		I’m a robotics engineer specializing in robot manipulation, reinforcement learning, and foundation models. I recently completed my Master’s degree in Robotics,
		with a minor in Data Science, at <a href="https://www.epfl.ch/">EPFL</a> (2025), conducting my thesis at <a href="https://www.mit.edu/">MIT</a> under the supervision of
		<a href="https://azizan.mit.edu/">Navid Azizan</a> and <a href="https://zt-yang.com/">Zhutian Yang</a>. 
		My work at <a href="https://www.mit.edu/">MIT</a> focused on long-horizon humanoid manipulation using vision-language models and imitation learning.</p>
		<p>
		Throughout my Master’s, I worked on deploying
		learning-based control systems in both simulation and real-world robots, including quadrupeds and humanoids, collaborating with 
		<a href="https://www.epfl.ch/labs/biorob/people/ijspeert/">Auke Ijspeert</a> as well. Prior to that, I earned my BSc in Mechanical Engineering 
		from <a href="https://www.epfl.ch/">EPFL</a> (2022). I’m particularly interested in building intelligent systems that can reason, plan, and act over extended tasks, 
		and I’m open to opportunities in robotics research and applied AI.
                </p>
                <p style="text-align:center">
                  <a href="mailto:andre.schakkal@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/CVAndreSchakkal.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=L1sCqAkAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/AndreSchakkal/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/AndreSchakkal.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/AndreSchakkal.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			  
<tr onmouseout="longhorizon_stop()" onmouseover="longhorizon_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='longhorizon_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/longhorizon.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/longhorizon.jpg' width="160">
        </div>
        <script type="text/javascript">
          function longhorizon_start() {
            document.getElementById('longhorizon_image').style.opacity = "1";
          }

          function longhorizon_stop() {
            document.getElementById('longhorizon_image').style.opacity = "0";
          }
          longhorizon_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
			Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation
          <!-- <span class="strong">Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation</span> -->
        </strong>
        <br>
		<strong>André Schakkal</strong>,
        <a href="https://benzandonati.co.uk/">Ben Zandonati</a>,
        <a href="https://zt-yang.com/">Zhutian Yang</a>,
        <a href="https://azizan.mit.edu/">Navid Azizan</a>
        <br>
        <em>2025 Robotics: Science and Systems Conference (RSS) Workshop on Robot Planning in the Era of Foundation Models</em>, 2025
        <br>
        <a href="https://vlp-humanoid.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2506.22827">paper</a>
        <p></p>
        <p>
		This work presents a hierarchical framework that enables humanoid robots to autonomously execute multi-step manipulation tasks. The system combines a low-level RL controller
		for whole-body motion tracking, mid-level imitation-learned skills for generating action-specific motion targets, and a high-level vision-language module that sequences skills 
		and monitors execution in real time. Tested on a Unitree G1 humanoid performing a pick-and-place task, the system achieves a 73% success rate.
		<!-- This work introduces a hierarchical framework for multi-step humanoid manipulation. The system integrates three layers: (1) a low-level RL controller that 
		tracks whole-body motion targets; (2) a mid-level set of skill policies trained via imitation learning that produce motion targets for different steps of a task; and 
		(3) a high-level vision-language module that selects and monitors skills using pretrained VLMs.         -->
		</p>
      </td>
    </tr>
	
    <tr onmouseout="inverse_stop()" onmouseover="inverse_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='inverse_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/inverse.mp4" type="video/mp4">
          Your browser does not support the video tag.

          </video></div>
          <img src='images/inverse.jpg' width="160">
        </div>
        <script type="text/javascript">
          function bolt3d_start() {
            document.getElementById('inverse_image').style.opacity = "1";
          }

          function bolt3d_stop() {
            document.getElementById('inverse_image').style.opacity = "0";
          }
          inverse_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
		Learning the Inverse Hitting Problem
        </strong>
        <br>
        <a href="https://harshitk95.github.io/">Harshit Khurana</a>,
        <a href="https://jameshermus.github.io/">James Hermus</a>,
        <a href="https://www.linkedin.com/in/maxime-gautier-00/">Maxime Gautier</a>,
		<strong>André Schakkal</strong>,
        <a href="https://people.epfl.ch/aude.billard">Aude Billard</a>
        <br>
        <em>2025 IEEE Robotics and Automation Letters (RA-L)</em>, 2025
        <br>
        <a href="https://ieeexplore.ieee.org/document/11071941">paper</a>
        <p></p>
        <p>
			This work presents a data-driven framework for impulsive object manipulation, where robots move objects by hitting them. Using a dual-arm air-hockey setup, the system learns the relationship between hitting force
			and object motion via a Gaussian Mixture Model and predicts impacts with an Impact-Aware Extended Kalman Filter. The approach enables collaborative, long-range object placement through sequential, golf-like hits.
			
			<!-- This work introduces a data-driven framework for modeling and planning impulsive object manipulation—where robots move objects by hitting rather than pushing or grasping. Using a dual-arm “air-hockey” setup,
			the system autonomously collects data as two robot arms exchange impacts on an object. An Impact-Aware Extended Kalman Filter (IA-EKF) predicts post-impact motion online, while a Gaussian Mixture Model (GMM)
			learns the stochastic relationship between hitting force and object displacement. The learned model enables robots to plan sequential, golf-like hits to move objects beyond their individual reach, demonstrating
			collaborative long-range object placement. -->
        </p>
      </td>
    </tr>


  <tr onmouseout="quad_stop()" onmouseover="quad_start()">
      <!-- <td style="padding:16px;width:20%;vertical-align:middle"> -->
	  <td style="padding:16px;width:20%;vertical-align:middle; text-align:center;">
        <div class="one">
          <div class="two" id='quad_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/quad.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/quad.jpg' width="160">
        </div>
        <script type="text/javascript">
          function quad_start() {
            document.getElementById('quad_image').style.opacity = "1";
          }

          function quad_stop() {
            document.getElementById('quad_image').style.opacity = "0";
          }
          quad_stop()
        </script>
	  </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
		Dynamic Object Catching with Quadruped Robot Front Legs
        </strong>
        <br>
				<strong>André Schakkal</strong>, 
				<a href="https://www.linkedin.com/in/guillaume-bellegarda/">Guillaume Bellegarda</a>,
				<a href="https://www.epfl.ch/labs/biorob/people/ijspeert/">Auke Ijspeert</a>
				<br>
        <em>2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 
        <br>
        <a href="https://arxiv.org/abs/2410.08065v1">paper</a>
		/
		<a href="https://www.youtube.com/watch?v=sm7RdxRfIYg/">video</a>
        <p></p>
        <p>
			This work enables a Unitree Go1 quadruped robot to catch thrown objects using its front legs while standing on its rear legs. Using an onboard camera 
			and a fine-tuned YOLOv8 model, the robot detects objects, predicts their trajectories, and executes a precise catching maneuver. A Gaussian
			Mixture Model (GMM) identifies optimal catching positions, enabling an 80% success rate across diverse throws.
			<!-- This paper presents a framework for dynamic object catching using a quadruped robot's front legs while it stands on its rear legs. 
			The system integrates computer vision, trajectory prediction, and leg control to enable the quadruped to visually detect, track, and 
			successfully catch a thrown object using an onboard camera. -->
        </p>
      </td>
    </tr>


    <!-- <tr onmouseout="synergizing_stop()" onmouseover="synergizing_start()" bgcolor="#ffffd0"> -->
    <tr onmouseout="synergizing_stop()" onmouseover="synergizing_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='synergizing_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/synergizing.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/synergizing.jpg' width="160">
        </div>
        <script type="text/javascript">
          function synergizing_start() {
            document.getElementById('synergizing_image').style.opacity = "1";
          }

          function synergizing_stop() {
            document.getElementById('synergizing_image').style.opacity = "0";
          }
          synergizing_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
			Synergizing Natural Language Towards Enhanced Shared Autonomy
        </strong>
        <br>
				<a href="https://shalutharajapakshe.com/">Shalutha Rajapakshe</a>,
				<a href="https://www.linkedin.com/in/atharva-dastenavar/">Atharva Dastenavar</a>,
				<strong>André Schakkal</strong>,
				<a href="https://emmanuel-senft.github.io/">Emmanuel Senft</a>
        <br>
        <em>Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction (HRI)</em>, 2024
        <br>
        <a href="https://dl.acm.org/doi/10.1145/3610978.3640720">paper</a>
        <p></p>
        <p>
				 This work introduces a voice-based shared autonomy framework for assistive robots, enabling users to refine robot behavior through natural language instead of physical inputs like joysticks
			. A fine-tuned DistilBERT model interprets sequences of verbal commands to infer correction directions (e.g., left, right, up) and distinguish between environment-dependent and independent instructions.
			The lightweight model runs on a CPU, outperforming larger transformers on complex sequential commands, and lays the groundwork for voice-based corrections in shared autonomy systems.
        </p>
      </td>
    </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			  
<tr onmouseout="teleop_stop()" onmouseover="teleop_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='teleop_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/teleop.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/teleop.jpg' width="160">
        </div>
        <script type="text/javascript">
          function teleop_start() {
            document.getElementById('teleop_image').style.opacity = "1";
          }

          function teleop_stop() {
            document.getElementById('teleop_image').style.opacity = "0";
          }
          teleop_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
			Humanoid Teleoperation
        </strong>
        <!-- <br>
        <a href="https://vlp-humanoid.github.io/">project page</a> -->
        <p></p>
        <p>
		This project enables real-time human control of a humanoid robot using a single RGB camera. Human motions are captured and estimated in 3D with the HybrIK 
		pose estimation model and retargeted to the robot’s morphology. The resulting retargeted pose defines the robot’s desired motion, which is executed in real 
		time by an RL-based control module that generates the corresponding joint commands.     
		</p>
      </td>
    </tr>			

<tr onmouseout="pred_stop()" onmouseover="pred_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='pred_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/pred.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/pred.jpg' width="160">
        </div>
        <script type="text/javascript">
          function pred_start() {
            document.getElementById('pred_image').style.opacity = "1";
          }

          function pred_stop() {
            document.getElementById('pred_image').style.opacity = "0";
          }
          pred_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
			Multimodal Human Trajectory Prediction
        </strong>
        <br>
        <a href="https://www.youtube.com/watch?v=f9-qefGN_VM">video</a>
        <p></p>
        <p>
		This project focuses on multimodal human trajectory prediction using the Nuscenes dataset. We adapt the Y-Net deep learning architecture to predict multiple plausible future trajectories
		for pedestrians based on past observations. The model accounts for the inherent uncertainty and diversity of human motion by generating probabilistic predictions, enabling it to capture different possible paths.
		</p>
      </td>
    </tr>					


<tr onmouseout="bounce_stop()" onmouseover="bounce_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bounce_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/bounce.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/bounce.jpg' width="160">
        </div>
        <script type="text/javascript">
          function bounce_start() {
            document.getElementById('bounce_image').style.opacity = "1";
          }

          function bounce_stop() {
            document.getElementById('bounce_image').style.opacity = "0";
          }
          bounce_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
			The Bouncing Table
        </strong>
        <br>
        <a href="https://www.youtube.com/shorts/fIEaFoTWYhU">video</a>
        <p></p>
        <p>
		This project showcases an autonomous motorized table designed to keep a ping pong ball bouncing continuously. Using a three-arm mechanical design driven by high-torque servomotors,
		the system reacts in real time to the ball’s motion. A high-speed camera tracks the ball, LabVIEW processes its position, and an Arduino-controlled mechanism adjusts the table’s 
		inclination and timing to maintain continuous bounces.  
		</p>
      </td>
    </tr>	
			  

      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/bert.jpg' width="160">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
			Tweet Sentiment Classification Challenge
        </strong>
        <!-- <br>
        <a href="https://vlp-humanoid.github.io/">project page</a> -->
        <p></p>
        <p>
		This project involved using BERTweet, a transformer pretrained on large-scale Twitter data, and fine-tuning it for binary sentiment classification.
		The model achieved 92.1% accuracy, the highest since the challenge began in 2019 in the CS 433 Machine Learning course at EPFL.  
		</p>
      </td>
    </tr>		

      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/characters.jpg' width="160">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>
			Actor Fame and Diversity Analysis in Cinema 
        </strong>
        <br>
        <a href="https://andreschakkal.github.io/ada-actor-website/">project page</a>
        <p></p>
        <p>
		This project performs data analysis on the <a href="https://www.cs.cmu.edu/~ark/personas/">CMU Movie Dataset</a> to tell a data-driven story. It examines actor recognition, gender, and ethnic representation in film over time, quantifying actor prominence using 
		a “Recognition Coefficient” that combines movie revenue, ratings, and role importance. The analysis highlights how major social movements, such as the Feminist and Civil Rights movements, shaped visibility 
		and recognition of female and minority actors, revealing trends in inclusivity and representation in cinema.
		</p>
      </td>
    </tr>		



		
			  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Honors and Awards</h2>
			    <ul>
			    <li> Faulhaber Best Master Thesis Award (2025)</li>
			    <li> EPFL Master Excellence Fellowship (2022-2025)</li>
			    </ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Industrial Experience</h2>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <a href="https://www.logitech.com/"> Logitech</a>,
              <br>
              Machine Learning Engineering Intern • Februaty 2024 to August 2024
              </br>
            </td>
            <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/logitech_logo.png" height="60"></td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <a href="https://www.lemo.com/"> Lemo</a>, Robotics Engineering Intern
              <br>
               July 2023 to August 2023
              </br>
            </td>
            <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/lemo_logo.png" height="60"></td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:75%;vertical-align:middle">
              <a href="https://www.vacheron-constantin.com/"> Vacheron Constantin </a>, Robotics Engineer 
              <br>
               December 2022 to June 2023
              </br>
            </td>
            <td style="padding:0px 20px 0px 20px;width:10%;vertical-align:middle"><img src="images/vacheron_logo.png" height="60"></td>
          </tr>
        </tbody></table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="1.5">
    Template modified from <a href="http://www.cs.berkeley.edu/~barron/">this</a>
    </font></p></td></tr>
</table>
